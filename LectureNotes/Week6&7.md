# 6.036 Week6

这两节课的内容主要是一些有关神经网络的基础概念，直接看书就行

## 1. 神经网络的定义&如何训练神经网络

神经网络的定义其实并不复杂，直接看 handout 就行。

神经网络/深度学习其实是一个很老的概念，上世纪四十年代就已经被提出。但在它最初诞生的时候由于不存在一个好的方法来进行训练所以并没有特别受到关注。在反向传播被提出后，神经网络又再次兴盛，但由于训练的算量依然很大，训练速度缓慢所以人们关于它的兴趣又逐渐磨灭。神经网络在我们当下又再次兴起得益于许多训练优化手段的成熟，以及训练结果收敛性的保障。

反向传播听起来很吊，但其实就是一个在神经网络系统中求各个参数与损失函数之间偏导的方法，虽然我不会，但如果你的数学功底很牛也可以去手推一遍公式。

## 2. 训练神经网络时的优化手段

训练神经网络时可以通过许多手段来进行训练的优化，从而加快训练速度，这里以梯度下降的优化手段作为讲解

1. batch gradient decent --> mini batch --> stochastic gradient decent 在计算量和每次迭代的梯度精准性之间权衡
2. 优化步长，比如 momentum 不光考虑某点上的梯度，也考虑之前的梯度；Adadelta 在梯度"陡/缓"时放缩步长；以及将这两点都考虑到的 Adam 方法

## 3. 正则化神经网络

通常来说神经网络的正则化并不是一个比较大的问题，当然神经网络的正则化有一定手段

1. Methods related to ridge regression: early stopping, weight decay, and adding noise to the training data.
2. Dropout
3. Batch Normalization
