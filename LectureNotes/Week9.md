
# 6.036 Week9

不同于在这一节之前的几个机器学习模型，之后几节课的模型具有新的特点:

1. 数据集中的数据可以由序列划分，模型需要感知一个序列中各个数据对的顺序，而不是将其当作孤立的输入输出
2. 损失函数会受 agent 与环境之间的互动影响
3. 会介绍非监督学习，也就是输送对于数据集的输入，我们不知道应该输出什么

这一节主要介绍 State Machines 和 Markov Decision Model(MDP)，作为之后模型学习的概念基础

## State Machines

比较简单，概念比较多，直接看 handout，里面讲得很详细

## MDP

区别于简单的状态机模型，在 MDP 这一部分里面引入了一些新的概念

1. 转移函数是随机的，是在某个状态下采取某个 action 之后下一个状态的概率分布
2. 引入了 reward function，用于求解在某个状态下的策略，用于在之后的序列里获得最大的期望收益
3. 引入了 discount function，用于求解在 infinite horizon 情况下的最佳策略

还有一些其他比较琐碎的概念，这里就不多说了，直接看 handout

在本章的最后，提出了一种迭代式的求解 Q 的方法
